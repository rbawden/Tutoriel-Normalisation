{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rbawden/Tutoriel-Normalisation/blob/main/Tutoriel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations sur la normalisation du français moderne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup de l'environnement, téléchargement des fichiers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installer les paquets python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fairseq@git+git://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52 \n",
    "!pip install sentencepiece sacrebleu hydra-core omegaconf==2.0.5 gdown==4.2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger les données et les modèles depuis Google Drive et les structurer dans les dossiers `data/`, `models/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/drive/folders/1h-qSnPBPZFZQ_kqWIBMhkkFS-6C2b10H?usp=sharing -O data-models --folder\n",
    "!mv data-models/structure_files.sh ./; bash structure_files.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Préparation des données à normaliser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour (i) lire le contenu d'un fichier ligne par ligne et (ii) les lire depuis un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lire un fichier ligne par ligne\n",
    "def read_file(filename):\n",
    "  list_sents = []\n",
    "  with open(filename) as fp:\n",
    "    for line in fp:\n",
    "      list_sents.append(line.strip())\n",
    "  return list_sents\n",
    "\n",
    "# écrire une liste de phrases dans un fichier\n",
    "def write_file(list_sents, filename):\n",
    "    with open(filename, 'w') as fp:\n",
    "        for sent in list_sents:\n",
    "            fp.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire le contenu du texte source à normaliser (`dev.src`) et le texte cible ('de référence'), c'est-à-dire le texte correctement normalisé (`dev.trg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_src = read_file('data/dev.src')\n",
    "dev_trg = read_file('data/dev.trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser le début des textes sources (src) et cibles (trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('src = ', dev_src[i])\n",
    "    print('trg = ', dev_trg[i])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger le modèle de segmentation en sous-mots (`bpe_joint_1000.model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_1000.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer le modèle de segmentation sur les données à normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_src_sp = spm.encode(dev_src, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire le texte pre-traité dans un fichier `dev.sp.src`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file([' '.join(phrase) for phrase in dev_src_sp], 'data/dev.sp.src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser le début du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_src_sp[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir une fonction pour détokeniser une liste de phrases (pour plus tard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sp(list_sents):\n",
    "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser à quoi ressemble le texte détokenisé (Spoiler: il devrait ressembler au texte de départ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_sp(dev_src_sp[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Appliquer le modèle de normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer le modèle de normalisation sur le début des données pre-traitées (ça prend moins de temps pour tester que normaliser tout le texte)\n",
    "\n",
    "Il y aura un message \"UserWarning\", mais vous pouvez l'ignorer - ce n'est pas grave.\n",
    "\n",
    "Explications:\n",
    "- `head -n 10` affiche les 10 premières phrases\n",
    "- ces 10 premières lignes sont donné à fairseq-interactive\n",
    "- le résultat va dans `data/dev.sp.norm.trg.10.output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 10 data/dev.sp.src | fairseq-interactive models/norm/ --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/dev.sp.norm.trg.10.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sortie de fairseq-interactive donne quelque chose comme ceci:\n",
    "\n",
    "```\n",
    "S-0     ▁1 .\n",
    "H-0     -0.00011481383990030736 ▁1 .\n",
    "P-0     -0.0000 -0.0003 -0.0000\n",
    "S-1     ▁1 . ▁Q V e ▁cette ▁prop ost ion , ▁qu ' vn ▁esp ace ▁est ▁v ui d é , ▁re p u gne ▁au ▁sens ▁comm un .\n",
    "H-1     -0.039981111884117126   ▁1 . ▁Q U e ▁cette ▁prop ost ion , ▁qu ' un ▁esp ace ▁est ▁v ui d é , ▁rép u gne ▁au ▁sens ▁comm un .\n",
    "P-1     -0.0000 -0.0000 -0.0043 -0.0632 -0.0006 -0.0000 -0.0001 -0.9353 -0.0001 -0.0012 -0.0000 0.0000 -0.0001 -0.0078 -0.0070 -0.0000 -0.0022 -0.1168 -0.0001 -0.0000 -0.0000 -0.0389 -0.0157 -0.0053 -0.0000 -0.0000 -0.0001 -0.0000 -0.0004 -0.0000\n",
    "S-2     ▁1 . ▁Q V e ▁tous ▁les ▁cor p s ▁ont ▁re p u gn ance ▁à ▁se ▁se p are r ▁l ' vn ▁de ▁l ' autre , ▁& ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter u al le ;\n",
    "W-2     0.682   seconds\n",
    "H-2     -0.019450930878520012   ▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter v és le ;\n",
    "D-2     -0.019450930878520012   ▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter v és le ;\n",
    "P-2     -0.0000 -0.0001 -0.0040 -0.1684 -0.0004 -0.0000 -0.0000 -0.0000 -0.0007 -0.0000 -0.0001 -0.1220 -0.0063 -0.0002 -0.0137 -0.0000 -0.0000 -0.0002 -0.0001 -0.0248 -0.0022 -0.0003 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 -0.0002 -0.0001 -0.0000 -0.0000 -0.0000 -0.0000 -0.0383 -0.0173 -0.0006 -0.0000 -0.0000 -0.0000 -0.0066 -0.0016 -0.4856 -0.0007 -0.0002 -0.0000\n",
    "```\n",
    "\n",
    "Les informations intéressantes pour l'exemple `i`:\n",
    "\n",
    "- S-i: le texte source\n",
    "- H-i: le score de l'hypothèse et l'hypothèse du modèle (c'est-à-dire la prédiction)\n",
    "- P-i: les scores de chaque sous-token produit par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour extraire l'hypothèse de ce fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hypothesis(filename):\n",
    "    outputs = []\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
    "            if 'H-' in line:\n",
    "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
    "                outputs.append(line.strip().split('\\t')[2])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les hypothèses du fichier produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_norm_10 = extract_hypothesis('data/dev.sp.norm.trg.10.output')\n",
    "dev_norm_10[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-traiter le texte avec la fonction précedemment définie (dé-segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_norm_10_postproc = decode_sp(dev_norm_10)\n",
    "dev_norm_10_postproc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire le résultat dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(dev_norm_10_postproc, 'data/dev.norm.10.trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation du résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLEU: le métrique d'évaluation le plus fréquemment utilisé en traduction automatique\n",
    "- ChrF: CharacterF score (like BLEU but based on n-grams of characters)\n",
    "- TER: translation edit rate\n",
    "\n",
    "Attention : puisque nous avons seulement normalisé 10 phrases, il faut seulement comparer contre les 10 première phrases de référence. En réalité, il faudrait calculer ces scores sur un plus grand nombre de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "bleu = BLEU()\n",
    "bleu.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = CHRF()\n",
    "chrf.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ter = TER()\n",
    "ter.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une évaluation plus adaptée : la précision au niveau de chaque mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
    "!head -n 10 data/dev.trg > data/dev.10.trg\n",
    "align_dev_norm_10 = align.align('data/dev.10.trg', 'data/dev.norm.10.trg')\n",
    "\n",
    "print(align_dev_norm_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat de l'alignement est une liste de phrases, où chaque mot de la phrase est comme suit:\n",
    "\n",
    "- le mot tout seul s'il est pareil dans les deux textes (ex : `QUe`)\n",
    "- le mot du premier document et le mot du deuxième document, séparé par \">\" s'ils sont différents (ex : proposition>propostion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diff = 0\n",
    "total = 0\n",
    "for sentence in align_dev_norm_10:\n",
    "    for word in sentence:\n",
    "        if '>' in word:\n",
    "            num_diff += 1\n",
    "        total += 1\n",
    "print('Accuracy = ' + str((total - num_diff)/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tester le modèle de dénormalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparer les données normalisées qui vont être dénormalisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_trg_sp = spm.encode(dev_trg, out_type=str) # tokenise the sentence into subtokens\n",
    "decade_token = '▁<decade=162> ' # special token indicating the decade\n",
    "write_file([' '.join([decade_token] + phrase) for phrase in dev_trg_sp], 'data/dev.sp.trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliser le texte\n",
    "\n",
    "(10 premières phrases seulement. Vous pouvez faire plus de phrases en modifiant le 10. Vous pouvez tout normaliser en changeant `head -n 10` en `cat`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 data/dev.sp.trg | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/dev.sp.denorm.src.10.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-traiter la sortie du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_denorm_10 = extract_hypothesis('data/dev.sp.denorm.src.10.output')\n",
    "dev_denorm_10_postproc = decode_sp(dev_denorm_10)\n",
    "write_file(dev_denorm_10_postproc, 'data/dev.sp.denorm.10.src')\n",
    "dev_denorm_10_postproc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a pas mal d'étapes, donc pour faciliter le traitement, voici une fonction qui prend en entrée une liste de phrases et qui fait tout :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalise(sents, decade):\n",
    "    assert int(decade) >=1600 and int(decade) < 1700, 'Your decade must be between 1600 and 1690'\n",
    "    # generate temporary file\n",
    "    filetmp = 'data/tmp_denorm.sp.trg.tmp'\n",
    "    # preprocessing\n",
    "    input_sp = spm.encode(sents, out_type=str)\n",
    "    # add decade token to each sentence\n",
    "    decade_token = '▁<decade=' + str(decade)[:3] + '>'\n",
    "    input_sp_sents = [' '.join([decade_token] + sent) for sent in input_sp]\n",
    "    write_file(input_sp_sents, filetmp)\n",
    "    #print(\"preprocessed = \", input_sp_sents)\n",
    "    # denormalisation\n",
    "    !cat data/tmp_denorm.sp.trg.tmp | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/tmp_denorm.sp.trg.output 2> /tmp/dev\n",
    "    # postprocessing\n",
    "    outputs = extract_hypothesis('data/tmp_denorm.sp.trg.output')\n",
    "    outputs_postproc = decode_sp(outputs)\n",
    "    return outputs_postproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on peut la tester comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denormalise([\"Je ne savais pas qu'il faisait si beau.\"], 1640))\n",
    "print(denormalise([\"Je ne savais pas qu'il faisait si beau.\"], 1690))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voici une fonction similaire pour la normalisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(sents):\n",
    "    # generate temporary file\n",
    "    filetmp = 'data/tmp_norm.sp.src.tmp'\n",
    "    # preprocessing\n",
    "    input_sp = spm.encode(sents, out_type=str)\n",
    "    # add decade token to each sentence\n",
    "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
    "    write_file(input_sp_sents, filetmp)\n",
    "    #print(\"preprocessed = \", input_sp_sents)\n",
    "    # denormalisation\n",
    "    !cat data/tmp_norm.sp.src.tmp | fairseq-interactive models/norm --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/tmp_norm.sp.src.output 2> /tmp/dev\n",
    "    # postprocessing\n",
    "    outputs = extract_hypothesis('data/tmp_norm.sp.src.output')\n",
    "    outputs_postproc = decode_sp(outputs)\n",
    "    return outputs_postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise([\"1. QVe cette propostion, qu'vn espace est vuidé, repugne au sens commun.\",\n",
    "          \"Affectoit un mépris qui marquoit ſon eſtime,\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Quelques extensions (y compris du code à faire 👩🏻‍💻🧑🏽‍💻)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le modèle sur le texte entier\n",
    "\n",
    "Attention : normaliser le fichier entier prend environ 6 minutes en utilisant le GPU\n",
    "\n",
    "Si vous voulez juste l'évaluer et l'analyser (sans refaire la normalisation), le fichier entièrement normalisé est disponible ici: `data/dev.norm.full.trg` \n",
    "\n",
    "Ou vous pouvez normaliser une partie du fichier seulement (comme avant mais avec plus de phrases que 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_norm = normalise(dev_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(dev_norm, 'data/dev.norm.trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Refaire l'évaluation sur le texte entier (pas juste sur les 10 premières phrases) : BLEU, ChrF, TER et exactitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez refaire la normalisation pour le jeu de test maintenant (il se trouve dans `data/test.src`) et l'évaluer/l'analyser\n",
    "\n",
    "La normalisation complète se trouve dans `data/test.norm.full.trg` si vous ne voulez pas attendre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = read_file('data/test.src')\n",
    "test_norm = normalise(test_src)\n",
    "#test_norm = read_file('data/test.norm.full.trg') # pour utiliser le texte déjà normalisé, décommenter cette ligner et commenter la ligne précédente\n",
    "write_file(test_norm, 'data/test.norm.trg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi faire la même évaluation sur le texte source pour voir quels seraient les scores si on ne changeait rien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire une analyse qualitative de résultats (sur le dev et/ou le test)\n",
    "\n",
    "Complétez cette fonction pour afficher les différences les plus fréquentes.\n",
    "\n",
    "(Le calcul des alignements est lent, donc vous pouvez tester avec moins de phrases aussi, surtout lorsque vous tester simplement votre fonction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 100 data/dev.trg > data/dev.100.trg\n",
    "!head -n 100 data/dev.norm.full.trg > data/dev.norm.100.trg\n",
    "alignments_dev = align.align('data/dev.100.trg', 'data/dev.norm.100.trg')\n",
    "#alignments_dev = align.align('data/dev.trg', 'data/dev.norm.full.trg') # décommenter pour traiter le jeu de dev en entier\n",
    "\n",
    "\n",
    "# get alignments\n",
    "def print_most_frequent_diffs(alignments, show_n=10):\n",
    "    # TODO\n",
    "    return\n",
    "\n",
    "print_most_frequent_diffs(alignments_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire une analyse qualitative des différences entre un texte source et cible pour visualiser les corresondences de normalisation.\n",
    "\n",
    "Idéalement ceci se fait sur le jeu d'entraînement (`data/train.src` et `data/train.trg`), mais ça risque de prendre trop de temps pour ce TP, donc prenez une sous-partie des exemples (en utilisant `head` comme avant) !\n",
    "\n",
    "- lire les deux fichiers\n",
    "- appliquer l'alignement\n",
    "- utiliser la fonction `print_most_frequent_diffs` (précédemment utilisée pour comparer la prédiction contre la référence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un modèle baseline par règles. Comme ressources supplémentaires, vous avez un lexique de mots en français contemporain et quelques fonctions\n",
    "\n",
    "Idées possibles:\n",
    "\n",
    "- remplacer les caractères qui changent systématiquement en utilisant la fonction `replace(avant, après)` : `word = word.replace('ſ', 's')`\n",
    "- remplacer les caractères en utilisant les expressions régulières (si vous les connaissez) : `word = word.replace('vn(e?)', r'un\\1')`\n",
    "- parcourir les mots de la phrase et si le mot n'apparaît pas dans le lexique, trouver le mot du lexique qui est le plus similaire. Quelques idées de fonctions de similarité:\n",
    "  - la distance de levenshtein (la fonction est donnée ci-dessous)\n",
    "  - une fonction plus simple qui compare le nombre de caractères en commun entre les deux mots (à faire)\n",
    "  - il serait peut-être sage de normaliser cette dernière similarité par la longueur des mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a lexicon in the mlex format\n",
    "import gzip\n",
    "def read_lexicon(filename):\n",
    "    words = []\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            words.append(line.split('\\t')[0])\n",
    "    return words\n",
    "\n",
    "# calculate the levenshtein distance\n",
    "def similarity_levenshtein(word1, word2):\n",
    "    dist, matrix, backpointers = levenshtein('@' + word1, '@' + word2)\n",
    "    return dist\n",
    "\n",
    "# calculate the number of common characters between the two words\n",
    "def similarity_common_chars(word1, word2):\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = read_lexicon('data/lefff-3.4.mlex.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le lexique contient ' + str(len(lexicon)) + ' entrées.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that normalises a sentence given a function that normalises a word\n",
    "import utils\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "def normalise_sent(sent, normalise_word_function):\n",
    "    norm_sent = []\n",
    "    # go through the sentence word by word (the tokenisation function is very approximate here!)\n",
    "    for word in utils.basic_tokenise(sent).split():\n",
    "        norm_sent.append(normalise_word_function(word))\n",
    "    return utils.detokenise(' '.join(norm_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the word itself\n",
    "def return_word(word):\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration of how this could work (using just the function that returns the original word)\n",
    "normalise_word_function = return_word\n",
    "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that replaces the long s\n",
    "def replace_long_s(word):\n",
    "    word = word.replace('ſ', 's')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on peut tester avec une autre petite fonction qui ne fait que remplacer les ſ par s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_word_function = replace_long_s\n",
    "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez créer une fonction qui contient plus de remplacements (comme `word.replace(before, after)`).\n",
    "\n",
    "Parfois, un remplacement peut être contextuel, donc si vous connaissez les expressions régulières vous pouvez les utiliser aussi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replace_regex(word):\n",
    "    word = word.replace('ſ', 's')\n",
    "    word = re.sub(\"([Qq])v\", r'\\1u', word)\n",
    "    word = re.sub(\"([Qq])V\", r'\\1U', word)\n",
    "    word = re.sub(\"('?)vn(e?)\", r'\\1un\\2', word)\n",
    "    return word\n",
    "\n",
    "normalise_word_function = replace_regex\n",
    "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraîner un modèle de segmentation en sous-mots en avec le toolkit sentencepiece\n",
    "\n",
    "Ce sera un modèle joint - c'est-à-dire qu'il est entraîné pour segmenter la langue source et cible et ça permet de faire des sous-mots qui peuvent être partagés pour les deux langues. C'est surtout bien pour les langues proches lexicalement.\n",
    "\n",
    "La taille du vocabulaire ici est de 2000, mais ceci peut être changé. La taille du vocabulaire détermine combien on découpé le texte. Plus le vocabulaire est petit, plus le texte sera découpé, plus le vocabulaire est grand, moins le texte sera découpé (ça ressemblera plus à un découpage sur les blancs). Vous pouvez tester avec des tailles de vocabulaires différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the source and target sides of the training set to train a joint model (encourages lexical sharing between the units)\n",
    "!cat data/train.src data/train.trg > data/all_train.src-trg\n",
    "sentencepiece.SentencePieceTrainer.train(input='data/all_train.src-trg', \n",
    "                               model_prefix='data/bpe_joint_2000', \n",
    "                               vocab_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire les trois jeux (train, dev, test) et appliquer les pré-traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = read_file('data/train.src')\n",
    "train_trg = read_file('data/train.trg')\n",
    "dev_src = read_file('data/dev.src')\n",
    "dev_trg = read_file('data/dev.trg')\n",
    "test_src = read_file('data/test.src')\n",
    "test_trg = read_file('data/test.trg')\n",
    "\n",
    "# load newly trained sentencepiece models\n",
    "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_2000.model')\n",
    "\n",
    "# apply sentencepiece to each of the datasets\n",
    "train_src_sp = spm.encode(train_src, out_type=str)\n",
    "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
    "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
    "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
    "test_src_sp = spm.encode(test_src, out_type=str)\n",
    "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
    "\n",
    "# print out lengths (src and trg must be the same length for each type of set)\n",
    "print(len(dev_src_sp), len(train_trg_sp))\n",
    "print(len(dev_src_sp), len(dev_trg_sp))\n",
    "print(len(test_src_sp), len(test_trg_sp))\n",
    "\n",
    "# write them to file\n",
    "write_file([' '.join(sent) for sent in train_src_sp], 'data/train.sp2000.src')\n",
    "write_file([' '.join(sent) for sent in train_trg_sp], 'data/train.sp2000.trg')\n",
    "write_file([' '.join(sent) for sent in dev_src_sp], 'data/dev.sp2000.src')\n",
    "write_file([' '.join(sent) for sent in dev_trg_sp], 'data/dev.sp2000.trg')\n",
    "write_file([' '.join(sent) for sent in test_src_sp], 'data/test.sp2000.src')\n",
    "write_file([' '.join(sent) for sent in test_trg_sp], 'data/test.sp2000.trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraîner un modèle récurrent (de type LSTM) avec fairseq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut d'abord binariser les données pour rendre l'utilisation de données plus efficace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess --destdir data/data_norm_bin_2000/ \\\n",
    "                    -s trg -t src \\\n",
    "                    --trainpref data/train.sp2000 \\\n",
    "                    --validpref data/dev.sp2000 \\\n",
    "                    --testpref data/test.sp2000 \\\n",
    "                    --joined-dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on peut appeler fairseq-train avec les paramètres souhaités\n",
    "\n",
    "- Le modèle entraîné se trouvera dans `models/new_norm_lstm/` (l'option `--save-dir`).\n",
    "- En pratique, le modèle sera sauvegardé plusieurs fois pendant l'entraînement (des checkpoints). La fréquence de sauvegarde peut être choisi avec l'option `--save-interval` (tous les `n` epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty model folder to store the model in\n",
    "!mkdir models/new_norm_lstm\n",
    "\n",
    "# call fairseq-train\n",
    "!fairseq-train \\\n",
    "        data/data_norm_bin_2000 \\\n",
    "        --save-dir models/new_norm_lstm \\\n",
    "        --save-interval 1 --patience 12 \\\n",
    "        --arch lstm \\\n",
    "        --encoder-layers 3 --decoder-layers 3 \\\n",
    "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
    "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
    "        --dropout 0.3 \\\n",
    "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
    "        --lr 0.0001 --lr-scheduler inverse_sqrt \\\n",
    "        --warmup-updates 4000 \\\n",
    "        --share-all-embeddings \\\n",
    "        --max-tokens 3000 \\\n",
    "        --batch-size-valid 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraîner un modèle de type transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty model folder to store the model in\n",
    "!mkdir models/new_norm_lstm\n",
    "\n",
    "# call fairseq-train\n",
    "!fairseq-train \\\n",
    "        data/data_norm_bin_2000 \\\n",
    "        --save-dir models/new_norm_transformer \\\n",
    "        --save-interval 1 --patience 25 \\\n",
    "        --arch transformer \\\n",
    "        --encoder-layers 2 --decoder-layers 4 --encoder-attention-heads 4 \\\n",
    "        --encoder-embed-dim 256 --encoder-ffn-embed-dim 1024 --dropout 0.3 \\\n",
    "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
    "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
    "        --warmup-updates 4000 \\\n",
    "        --max-tokens 3000 --max-tokens 3000 \\\n",
    "        --share-all-embeddings --batch-size-valid 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8AweSe1ASxQbXHW5LKQ6B",
   "include_colab_link": true,
   "name": "Tutoriel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
